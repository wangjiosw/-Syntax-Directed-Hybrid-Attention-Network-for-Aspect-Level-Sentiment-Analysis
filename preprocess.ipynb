{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.dom.minidom\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(file_path):\n",
    "    \"\"\"\n",
    "    extract data from xml file\n",
    "    \"\"\"\n",
    "    \n",
    "    DOMTree = xml.dom.minidom.parse(file_path)\n",
    "    collection = DOMTree.documentElement\n",
    "    \n",
    "    data = []\n",
    "    sents = collection.getElementsByTagName(\"sentence\") \n",
    "    for sent in sents:\n",
    "      aspectTerms = sent.getElementsByTagName('aspectTerms')\n",
    "      if len(list(aspectTerms)):\n",
    "        \n",
    "        text = sent.getElementsByTagName(\"text\")[0]\n",
    "        temp = text.childNodes[0].data\n",
    "    \n",
    "        aspectTerm = aspectTerms[0].getElementsByTagName(\"aspectTerm\")\n",
    "        for ap in aspectTerm:\n",
    "          content = []\n",
    "          content.append(temp)\n",
    "          content.append(ap.getAttribute(\"term\"))\n",
    "          content.append(ap.getAttribute(\"polarity\"))\n",
    "          data.append(content)\n",
    "    \n",
    "    df = pd.DataFrame(data,columns=['text','target','label'])\n",
    "    df = df[df['label'] != 'conflict']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def saveData(input_path,save_path):\n",
    "    df = extractData(input_path)\n",
    "    df.to_csv(save_path,index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   target     label\n",
       "0               But the staff was so horrible to us.    staff  negative\n",
       "1  To be completely fair, the only redeeming fact...     food  positive\n",
       "2  The food is uniformly exceptional, with a very...     food  positive\n",
       "3  The food is uniformly exceptional, with a very...  kitchen  positive\n",
       "4  The food is uniformly exceptional, with a very...     menu   neutral"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = saveData('./DATA/Restaurants_Train.xml','./DATA/train.csv')\n",
    "test_df = saveData('./DATA/restaurants-trial.xml','./DATA/test.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from enchant.checker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food, ou,.()asdtsta-nssd'    din\"  g lit*tle 'perks' great 39pm 123sd23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'food ou asdtsta nssd din g lit tle perks great pm sd'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent = 'food, ou,.()asdtsta-nssd\\'    din\\\"  g lit*tle \\'perks\\' great 39pm 123sd23'\n",
    "print(sent)\n",
    "\n",
    "sent = re.sub(r'[0-9]','',sent)\n",
    "sent = re.sub(r'[0-9,:?.\\'\\\"!()*-]',' ',sent)\n",
    "sent = re.sub(r'  +',' ',sent)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutWords(content):\n",
    "    sents = nltk.sent_tokenize(content)\n",
    "    word = []\n",
    "    for sent in sents:\n",
    "        word.extend(nltk.word_tokenize(sent))\n",
    "\n",
    "    return word\n",
    "\n",
    "    \n",
    "\n",
    "chkr = SpellChecker(\"en_US\")\n",
    "# stop_words = stopwords.words('english')\n",
    "# for w in ['!',',','.','?','-s','-ly','</s>','s','(',')','\\'','\\\"','\\'']:\n",
    "#     stop_words.append(w)\n",
    "\n",
    "stop_words = []\n",
    "stop_words.extend(['!',',','.','?','-s','-ly','</s>','s','(',')','\\'','\\\"','-'])\n",
    "\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def pre(sent):\n",
    "    # To low case\n",
    "    sent = sent.lower()\n",
    "    \n",
    "    sent = sent.replace('\\'t', ' not')\n",
    "    sent = sent.replace('whats', 'what is')\n",
    "    sent = sent.replace('\\'ve', ' have')\n",
    "    sent = sent.replace('\\'m', ' am')\n",
    "    sent = sent.replace('\\'ll', ' will')\n",
    "    sent = re.sub(r'[0-9,:?.\\'\\\"!()*-]',' ',sent)\n",
    "    sent = re.sub(r'  +',' ',sent)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Spell check\n",
    "    chkr.set_text(sent)\n",
    "    \n",
    "    for err in chkr:\n",
    "        try:\n",
    "            sent = sent.replace(err.word,chkr.suggest(err.word)[0])\n",
    "        except IndexError:\n",
    "            continue\n",
    "    \n",
    "    word_list = cutWords(sent)\n",
    "    # filter stop words        \n",
    "#     filtered_words = [word for word in word_list if word not in stop_words]\n",
    "    \n",
    "    #Lemmatization\n",
    "    lwords = []\n",
    "    for w in word_list:\n",
    "        lwords.append(wnl.lemmatize(w))  \n",
    "    \n",
    "    \n",
    "    return str(\" \".join(lwords))\n",
    "\n",
    "\n",
    "\n",
    "def getLabel(result):\n",
    "    if result == 'positive':\n",
    "        return 1\n",
    "    elif result == 'neutral':\n",
    "        return 0\n",
    "    elif result == 'negative':\n",
    "        return -1\n",
    "    else:\n",
    "        print(result)\n",
    "        print('error type')\n",
    "        exit(1)\n",
    "  \n",
    "def savePreData(input_path,save_file_name):\n",
    "    \n",
    "#     dir_path = './DATA/{}'.format(save_file_name)\n",
    "#     if not os.path.exists(dir_path): \n",
    "#         os.makedirs(dir_path) \n",
    "        \n",
    "#     x_path = './DATA/{}/{}_x.npy'.format(save_file_name,save_file_name)\n",
    "#     target_path = './DATA/{}/{}_target.npy'.format(save_file_name,save_file_name)\n",
    "#     label_path = './DATA/{}/{}_label.npy'.format(save_file_name,save_file_name)\n",
    "\n",
    "    train_df = pd.read_csv(input_path,encoding='utf-8')\n",
    "\n",
    "    train_label = train_df.label    \n",
    "    train_label = train_label.apply(getLabel)\n",
    "    train_label = keras.utils.to_categorical(train_label, num_classes=3)\n",
    "    \n",
    "#     train_text = train_df.text\n",
    "#     train_target = train_df.target\n",
    "#     train_x = train_text.apply(pre2)\n",
    "#     train_target = train_target.apply(pre2)\n",
    "\n",
    "    train_df['text'] = train_df['text'].apply(pre)\n",
    "    train_df['target'] = train_df['target'].apply(pre)\n",
    "\n",
    "        \n",
    "\n",
    "#     np.save(x_path,train_x)\n",
    "#     np.save(target_path,train_target)\n",
    "#     np.save(label_path,train_label)\n",
    "\n",
    "    df_path = './DATA/pre_{}.csv'.format(save_file_name)\n",
    "    train_df.to_csv(df_path,index=False)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>but the staff wa so horrible to u</td>\n",
       "      <td>staff</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be completely fair the only redeeming facto...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the food is uniformly exceptional with a very ...</td>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   target     label\n",
       "0                  but the staff wa so horrible to u    staff  negative\n",
       "1  to be completely fair the only redeeming facto...     food  positive\n",
       "2  the food is uniformly exceptional with a very ...     food  positive\n",
       "3  the food is uniformly exceptional with a very ...  kitchen  positive\n",
       "4  the food is uniformly exceptional with a very ...     menu   neutral"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_df = savePreData('./DATA/train.csv','train')\n",
    "pre_test_df = savePreData('./DATA/test.csv','test')\n",
    "pre_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      not only wa the food outstanding but the littl...\n",
       "target                                                 food\n",
       "label                                              positive\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_train_df.iloc[5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from enchant.checker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "  \n",
    "# sent = \"The service is absolutely terrible\"\n",
    "# target = 'service'\n",
    "\n",
    "# def strPre(sent):\n",
    "#      # To low case\n",
    "#     sent = sent.lower()\n",
    "    \n",
    "#     # Spell check\n",
    "#     chkr.set_text(sent)\n",
    "    \n",
    "#     for err in chkr:\n",
    "#         try:\n",
    "#             sent = sent.replace(err.word,chkr.suggest(err.word)[0])\n",
    "#         except IndexError:\n",
    "#             continue\n",
    "#     return sent\n",
    "LSK = []\n",
    "\n",
    "def getLSK(sent,target):\n",
    "    \n",
    "    # sent = strPre(sent)\n",
    "    # target = strPre(target)\n",
    "\n",
    "    words = sent.split(\" \")\n",
    "\n",
    "    sent_len = len(words)\n",
    "\n",
    "    distance = np.zeros((sent_len,sent_len))\n",
    "\n",
    "    # sent = unicode(sent, \"utf-8\")\n",
    "    \n",
    "\n",
    "    doc = nlp(sent)\n",
    "    for token in doc:\n",
    "    #     print(token.text, token.head.text,\n",
    "    #           [child for child in token.children])\n",
    "        try:\n",
    "            index1 = words.index(token.text)\n",
    "        except ValueError:\n",
    "            print(sent)\n",
    "            print(target)\n",
    "            print('token: %s not find in snetence\\n'%(token))\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            index2 = words.index(token.head.text)\n",
    "            if token.text != token.head.text:\n",
    "                distance[index1,index2] = 1\n",
    "        except ValueError:\n",
    "            print(sent)\n",
    "            print(target)\n",
    "            print('token head: %s not find in sentence\\n'%(token.head.text))\n",
    "            continue\n",
    "            \n",
    "\n",
    "        for child in token.children:\n",
    "            try:\n",
    "                index3 = words.index(str(child))\n",
    "                distance[index1,index3] = 1\n",
    "            except ValueError:\n",
    "                print(sent)\n",
    "                print(target)\n",
    "                print('%s\\'s child: %s not find\\n'%(token,child))\n",
    "                continue\n",
    "    # print distance\n",
    "\n",
    "    lsk = np.zeros(sent_len)\n",
    "\n",
    "    all_target = str(target).split(\" \")\n",
    "    for t in all_target:\n",
    "        try:\n",
    "            row = words.index(t)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        lsk = distance[row,:]\n",
    "        for i,value in enumerate(lsk):\n",
    "            if value == 1:\n",
    "                lsk = lsk + distance[i,:]\n",
    "\n",
    "    for i,value in enumerate(lsk):\n",
    "        if value > 1:\n",
    "            lsk[i] = 1\n",
    "    LSK.append(list(lsk))\n",
    "    return True\n",
    "    \n",
    "def saveLSK(mode):\n",
    "    data_path = './DATA/pre_{}.csv'.format(mode)\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    df.apply(lambda row: getLSK(row['text'], row['target']),axis=1)\n",
    "    print(len(LSK))\n",
    "    \n",
    "    save_path = './DATA/{}/LSK.npy'.format(mode)\n",
    "    np.save(save_path,LSK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3608\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "LSK = []\n",
    "saveLSK('train')\n",
    "\n",
    "LSK = []\n",
    "saveLSK('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "glove_model_path = './GLOVE_MODEL/glove300d.txt'\n",
    "\n",
    "with open(glove_model_path, 'r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        c = line.split(\" \")[0]\n",
    "        corpus[c] = i\n",
    "    f.close()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to 0.31924 0.06316 -0.27858 0.2612 0.079248 -0.21462 -0.10495 0.15495 -0.03353 2.4834 -0.50904 0.08749 0.21426 0.22151 -0.25234 -0.097544 -0.1927 1.3606 -0.11592 -0.10383 0.21929 0.11997 -0.11063 0.14212 -0.16643 0.21815 0.0042086 -0.070012 -0.23532 -0.26518 0.031248 0.16669 -0.089777 0.20059 0.31614 -0.5583 0.075735 0.27635 0.12741 -0.18185 -0.12722 0.024686 -0.077233 -0.48998 0.020355 0.0039164 0.1215 0.089723 -0.078975 0.081443 -0.099087 -0.055621 0.10737 -0.0044042 0.48496 0.11717 -0.017329 0.109 -0.35558 0.051084 0.15714 0.17961 -0.29711 0.033645 -0.025792 -0.013931 -0.23 -0.040306 0.22282 -0.013544 0.011554 0.3911 0.26533 -0.31012 0.40539 -0.042975 0.020811 -0.33033 0.19573 -0.037958 0.10274 -0.0013581 -0.44505 0.077886 0.08511 -0.20285 -0.19481 0.056933 0.53105 0.034154 -0.56996 -0.18469 0.093403 0.28044 -0.23349 0.10938 -0.014288 -0.274 0.034196 -0.098479 0.13268 0.19437 0.13463 -0.099059 0.040324 -0.66272 0.3571 0.15429 0.18598 0.087542 0.080538 -0.25121 0.24155 0.1783 0.036011 -0.027677 0.21161 -0.29107 -0.0083456 0.11317 0.31064 -0.10693 -0.27367 -0.039785 0.039881 0.034462 -0.16518 0.16115 0.060826 0.3075 -0.22398 0.14619 -0.2661 0.49732 -0.13996 -0.24287 0.039469 -0.084495 -0.24315 0.070701 -1.0136 -0.21733 -0.36878 -0.24973 0.17472 -0.011592 0.068561 -0.090411 0.21878 -0.2639 0.11904 0.14285 -0.18707 -0.13474 -0.13232 -0.26553 0.22947 -0.018215 0.0067383 -0.1019 0.10053 -0.1127 -0.13295 0.15951 0.14906 -0.095578 0.26992 0.011057 0.056568 0.021386 0.20215 0.00048589 0.5336 -0.22947 0.29275 0.17378 0.25423 -0.10976 0.058816 0.014616 -0.04306 0.10732 -0.028149 -0.19181 0.1025 -0.063892 0.012737 -0.12913 0.015037 0.26562 -0.017049 -0.060716 -0.094919 0.017775 0.13221 0.1683 -0.19323 -0.17612 0.075506 0.18939 0.12508 -0.1988 -0.16017 -0.21092 0.46933 0.044747 0.098349 0.011637 0.22281 -0.010837 -0.04833 -0.47335 -0.36811 -0.13592 -0.15086 0.25416 0.069531 0.14211 -0.26703 -0.1259 0.12076 -0.26117 0.033024 -0.034398 -0.13968 0.13446 -0.16709 0.15002 -0.13724 0.091226 -0.27718 0.020098 0.26919 0.43016 0.094019 -0.085496 -0.25192 -0.11645 -0.039734 0.0046738 0.54178 -0.16636 0.34546 0.098501 0.47819 -0.38428 -0.3238 -0.14822 -0.47817 0.16704 -0.064505 0.11834 -0.3448 0.096891 0.32309 0.41471 0.19463 -0.20891 -0.12223 -0.058298 -0.20268 0.2948 0.043397 0.10112 0.27177 -0.52124 -0.073794 0.044808 0.41388 0.088782 0.62255 -0.072391 0.090129 0.15428 0.023163 -0.13028 0.061762 0.33803 -0.091581 0.21039 0.05108 0.19184 0.10444 0.2138 -0.35091 -0.23702 0.038399 -0.10031 0.18359 0.025178 -0.12977 0.3713 0.18888 -0.0042738 -0.10645 -0.2581 -0.044629 0.082745 0.097801 0.25045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import linecache\n",
    "print(linecache.getline(glove_model_path,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getline(line_num):\n",
    "#     # row num begin from 1 not 0\n",
    "#     if line_num < 1 :return ''\n",
    "#     for currline,line in enumerate(open(glove_model_path,'rU')):\n",
    "#         if currline == line_num -1 : return line\n",
    "#     return ''\n",
    "def getLine(line_num):\n",
    "    return linecache.getline(glove_model_path,line_num)\n",
    "\n",
    "def gloveVec(inp):\n",
    "    \"\"\"\n",
    "    transform each word(sentence) to Glove vector(300 dim)\n",
    "    \"\"\"\n",
    "    glove_vecs = [] \n",
    "    for sent in inp:\n",
    "        sent = sent.split(' ')\n",
    "        glove_vec = []\n",
    "        for w in sent:\n",
    "            try:\n",
    "                index = corpus[w]\n",
    "                s = getLine(index+1)\n",
    "                vec = s.split(\" \")[1:]\n",
    "                vec = list(np.array(vec).astype(np.float))\n",
    "            except KeyError:\n",
    "                vec = list(np.random.uniform(-0.01,0.01,(300)))\n",
    "            glove_vec.append(vec)\n",
    "                \n",
    "        glove_vecs.append(glove_vec)\n",
    "    return glove_vecs\n",
    "\n",
    "def gloveTargetVec(inp):\n",
    "    \"\"\"\n",
    "    transform each word(target) to Glove vector(300 dim)\n",
    "    \"\"\"\n",
    "    vec = np.zeros(300)\n",
    "\n",
    "    for w in inp:\n",
    "        try:\n",
    "            index = corpus[w]\n",
    "            s = getLine(index+1)\n",
    "            s = np.array(s.split(\" \")[1:])\n",
    "            s = s.astype(np.float)\n",
    "            vec = vec + s\n",
    "        except KeyError:\n",
    "            vec = vec + np.random.uniform(-0.01,0.01,(300))\n",
    "    \n",
    "    vec =  vec/len(inp)\n",
    "    return list(vec)\n",
    "\n",
    "def saveGloveVec(mode):\n",
    "    # mode = 'train' or 'test'\n",
    "#     train_x_path = './DATA/{}/{}_x.npy'.format(mode,mode)\n",
    "#     train_target_path = './DATA/{}/{}_target.npy'.format(mode,mode)\n",
    "    \n",
    "    path = './DATA/pre_{}.csv'.format(mode)\n",
    "    data = pd.read_csv(path)\n",
    "    train_x = data['text']\n",
    "    train_target = data['target']\n",
    "    \n",
    "#     train_x = np.load(train_x_path,allow_pickle=True)\n",
    "#     train_target = np.load(train_target_path,allow_pickle=True)\n",
    "\n",
    "    glove_vec = gloveVec(train_x)\n",
    "    \n",
    "    glove_vec_path = \"./DATA/{}/glove_vec.npy\".format(mode)\n",
    "    np.save(glove_vec_path,glove_vec)\n",
    "    \n",
    "    # golve_target_vec = train_target.apply(gloveTargetVec)\n",
    "    t_len = len(train_target)\n",
    "    golve_target_vec = []\n",
    "    for i in range(t_len):\n",
    "        vec = gloveTargetVec(train_target[i])\n",
    "        golve_target_vec.append(vec)\n",
    "    \n",
    "    glove_target_vec_path = \"./DATA/{}/glove_target_vec.npy\".format(mode)\n",
    "    np.save(glove_target_vec_path,golve_target_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveGloveVec('train')\n",
    "saveGloveVec('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
